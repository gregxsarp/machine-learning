{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "sprint9.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss6CuoXeLwK0"
      },
      "source": [
        "# import the dependencies\n",
        "import numpy as np"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpVpptBlKKop"
      },
      "source": [
        "#### [Problem 1] Creating a one-dimensional convolutional layer class that limits the number of channels to one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VVNhsV2KPw8"
      },
      "source": [
        "class SimpleConv1d():\n",
        "\n",
        "  def forward(self, X, W, b):\n",
        "    a = []\n",
        "    for i in range(len(W)-1):\n",
        "      a.append((np.matmul(X[i:i+len(W)],W))+b[0])\n",
        "    return np.array(a)\n",
        "\n",
        "  def backward(self, X, W, dA):\n",
        "    db = np.sum(dA)\n",
        "    dW = []\n",
        "    for i in range(len(W)):\n",
        "      dW.append(np.matmul(dA,X[i:i+len(dA)]))\n",
        "    dW = np.array(dW)\n",
        "    dX = []\n",
        "    new_W = np.insert(W[::-1],0,0)\n",
        "    new_W = np.append(new_W,0)\n",
        "    for i in range(len(new_W)-1):\n",
        "      dX.append(np.matmul(new_W[i:i+len(dA)],dA))\n",
        "    dX = np.array(dX[::-1])\n",
        "    return db, dW, dX"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZxoLZVBOrT3"
      },
      "source": [
        "#### [Problem 2] Output size calculation after one-dimensional convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_jRj2XUOs9c"
      },
      "source": [
        "def output_size_calc(input_size, f, padding=0, stride=1):\n",
        "  out_size = ((input_size+2*padding-f)/stride)+1\n",
        "  return int(out_size)"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGAWqwK3Pk9G"
      },
      "source": [
        "#### [Problem 3] Experiment of one-dimensional convolutional layer with small array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dfr7v4gwPn_o"
      },
      "source": [
        "X = np.array([1,2,3,4])\n",
        "W = np.array([3, 5, 7])\n",
        "b = np.array([1])\n",
        "delta_a = np.array([10, 20])"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc2QiV6vQEJx",
        "outputId": "8d837bff-e2ac-45d5-eb20-696671bcc9ff"
      },
      "source": [
        "s1dconv = SimpleConv1d()\n",
        "forward_prop = s1dconv.forward(X, W, b)\n",
        "db, dW, dX = s1dconv.backward(X, W, delta_a)\n",
        "print(forward_prop)\n",
        "print(db)\n",
        "print(dW)\n",
        "print(dX)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[35 50]\n",
            "30\n",
            "[ 50  80 110]\n",
            "[ 30 110 170 140]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk7S_ordR7gN"
      },
      "source": [
        "##### [Problem 4], [Problem 5], [Problem 6]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAigzJvhR-cV"
      },
      "source": [
        "class SimpleInitializer:\n",
        "\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "        \n",
        "    def W(self, *shape):\n",
        "        W = self.sigma * np.random.randn(*shape)\n",
        "        return W\n",
        "    \n",
        "    def b(self, *shape):\n",
        "        b = self.sigma * np.random.randn(*shape)\n",
        "        return b"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPff0xaqVhXi"
      },
      "source": [
        "class SGD:\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    def update(self, layer):\n",
        "        layer.W -= self.lr * layer.dW / len(layer.Z)\n",
        "        layer.b -= self.lr * layer.dB / len(layer.Z)\n",
        "        return layer"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPRxR7aNXXFE"
      },
      "source": [
        "class Adagrad:\n",
        "  def __init__(self,lr):\n",
        "    self.lr = lr\n",
        "    self.hW = 0\n",
        "    self.hb = 0\n",
        "\n",
        "  def update(self, layer):\n",
        "    self.hW += layer.dW*layer.dW\n",
        "    self.hb += layer.db*layer.db\n",
        "    layer.W -= self.lr * layer.dW\n",
        "    layer.b -= self.lr * layer.db\n",
        "    return layer"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEK0gPke1J05"
      },
      "source": [
        "class ReLU:\n",
        "    \n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.clip(A, 0, None)\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        return dZ * np.clip(np.sign(self.A), 0, None)"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9146h83t1KqQ"
      },
      "source": [
        "class Sigmoid():\n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def calc(self,X):\n",
        "    return 1/(1+np.exp(-X))\n",
        "\n",
        "class Tanh:\n",
        "  def forward(self, A):\n",
        "      self.A = A\n",
        "      return np.tanh(A)\n",
        "  \n",
        "  def backward(self, dZ):\n",
        "      return dZ * (1 - (np.tanh(self.A))**2)\n",
        "\n",
        "class Softmax:\n",
        "  def forward(self, X):\n",
        "      self.Z = np.exp(X) / np.sum(np.exp(X), axis=1).reshape(-1,1)\n",
        "      return self.Z\n",
        "\n",
        "  def backward(self, Y):\n",
        "      self.loss = self.loss_func(Y)\n",
        "      return self.Z - Y\n",
        "\n",
        "  def loss_func(self, Y, Z=None):\n",
        "      if Z is None:\n",
        "          Z = self.Z\n",
        "      return (-1)*np.average(np.sum(Y*np.log(Z), axis=1))\n",
        "  \n",
        "  def calc(self,X):\n",
        "    return np.exp(X) / np.sum(np.exp(X), axis=1).reshape(-1, 1)"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSR_aDlA6-z2"
      },
      "source": [
        "import math\n",
        "\n",
        "class XavierInitializer:\n",
        "    \n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        self.sigma = math.sqrt(1 / n_nodes1)\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "    \n",
        "    def b(self, n_nodes2):\n",
        "        b = self.sigma * np.random.randn(n_nodes2)\n",
        "        return b"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn9xcbnmXons"
      },
      "source": [
        "# mini batch\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "Iterator to get a mini-batch\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : The following forms of ndarray, shape (n_samples, n_features)\n",
        "      Training data\n",
        "    y : The following form of ndarray, shape (n_samples, 1)\n",
        "      Correct answer value\n",
        "    batch_size : int\n",
        "      Batch size\n",
        "    seed : int\n",
        "      NumPy random number seed\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRgvWXe7q8V1"
      },
      "source": [
        "class FC:\n",
        "\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.b = initializer.b(n_nodes2)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        A = X@self.W + self.b\n",
        "        return A\n",
        "    \n",
        "    def backward(self, dA):\n",
        "        dZ = dA@self.W.T\n",
        "        self.dB = np.sum(dA, axis=0)\n",
        "        self.dW = self.X.T@dA\n",
        "        self.optimizer.update(self)\n",
        "        return dZ"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieWDGkewYX0v"
      },
      "source": [
        "class Conv1d:\n",
        "  \n",
        "  def __init__(self, batch_size, initializer, optimizer, input_size_channel=1, output_size_channel=1, padding=0):\n",
        "    self.batch_size = batch_size\n",
        "    self.input_size_channel = input_size_channel\n",
        "    self.output_size_channel = output_size_channel\n",
        "    self.padding = padding\n",
        "    self.initializer =  initializer\n",
        "    self.optimizer = optimizer\n",
        "    self.W = initializer.W(input_size_channel, output_size_channel, batch_size)\n",
        "    self.b = initializer.b(output_size_channel)\n",
        "\n",
        "  def forward(self, X):\n",
        "    self.n_in = X.shape[-1]\n",
        "    self.n_out = output_size_calc(self.n_in, self.batch_size, self.padding)\n",
        "    X = X.reshape(self.input_size_channel, self.n_in)\n",
        "    self.X = np.pad(X, ((0,0), ((self.batch_size-1), 0)))\n",
        "    self.X1 = np.zeros((self.input_size_channel, self.batch_size, self.n_in+(self.batch_size-1)))\n",
        "    for i in range(self.batch_size):\n",
        "        self.X1[:, i] = np.roll(self.X, -i, axis=-1)\n",
        "    A = np.sum(self.X1[:, :, self.batch_size-1-self.padding:self.n_in+self.padding]*self.W[:, :, :, np.newaxis], axis=(1, 2)) + self.b.reshape(-1,1)\n",
        "    return A\n",
        "\n",
        "  def backward(self, dA):\n",
        "    self.dW = np.sum(np.dot(dA, self.X1[:, :, self.batch_size-1-self.padding:self.input_size+self.padding, np.newaxis]), axis=-1)\n",
        "    self.dB = np.sum(dA, axis=1)\n",
        "    self.dA = np.pad(dA, ((0,0), (0, (self.batch_size-1))))\n",
        "    self.dA1 = np.zeros((self.output_size_channel, self.batch_size, self.dA.shape[-1]))\n",
        "    for i in range(self.batch_size):\n",
        "        self.dA1[:, i] = np.roll(self.dA, i, axis=-1)\n",
        "    dX = np.sum(self.W@self.dA1, axis=0)\n",
        "    self.optimizer.update(self)\n",
        "    return dX"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d7ESqekbgOA"
      },
      "source": [
        "model_1 = Conv1d(3, SimpleInitializer(0.01), SGD(0.01), 2, 3, 0)"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LChMEuhkdpi-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45f049a4-d502-4f3f-bfbb-3582993d6b0f"
      },
      "source": [
        "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) \n",
        "model_1.W = np.ones((3, 2, 3), dtype=float)\n",
        "model_1.b = np.array([1, 2, 3], dtype=float)\n",
        "model_1_forward = model_1.forward(x)\n",
        "model_1_forward"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[16., 22.],\n",
              "       [17., 23.],\n",
              "       [18., 24.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAD9m8HNmlVc"
      },
      "source": [
        "#### [Problem 7] (Advance assignment) Arbitrary number of strides"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNscJEPamiPP"
      },
      "source": [
        "class AdvConv1d:\n",
        "\n",
        "  def __init__(self, b_size, initializer, optimizer, n_in_channels=1, n_out_channels=1, pa=0, stride=1):\n",
        "    self.b_size = b_size\n",
        "    self.optimizer = optimizer\n",
        "    self.pa = pa\n",
        "    self.stride = stride\n",
        "    self.W = initializer.W(n_out_channels, n_in_channels, b_size)\n",
        "    self.b = initializer.b(n_out_channels)\n",
        "    self.n_in_channels = n_in_channels\n",
        "    self.n_out_channels = n_out_channels\n",
        "    self.n_out = None\n",
        "        \n",
        "  def forward(self, X):\n",
        "    self.n_samples = X.shape[0]\n",
        "    self.n_in = X.shape[-1]\n",
        "    self.n_out = output_size_calc(self.n_in, self.b_size, self.pa, self.stride)\n",
        "    X = X.reshape(self.n_samples, self.n_in_channels, self.n_in)\n",
        "    self.X = np.pad(X, ((0,0), (0,0), ((self.b_size-1), 0)))\n",
        "    self.X1 = np.zeros((self.n_samples, self.n_in_channels, self.b_size, self.n_in+(self.b_size-1)))\n",
        "    for i in range(self.b_size):\n",
        "        self.X1[:, :, i] = np.roll(self.X, -i, axis=-1)\n",
        "    A = np.sum(self.X1[:, np.newaxis, :, :, self.b_size-1-self.pa:self.n_in+self.pa:self.stride]*self.W[:, :, :, np.newaxis], axis=(2, 3)) + self.b.reshape(-1,1)\n",
        "    return A\n",
        "  \n",
        "  def backward(self, dA):\n",
        "    self.dW = np.sum(dA[:, :, np.newaxis, np.newaxis]*self.X1[:, np.newaxis, :, :, self.b_size-1-self.pa:self.n_in+self.pa:self.stride], axis=(0, -1))\n",
        "    self.dB = np.sum(dA, axis=(0, -1))\n",
        "    self.dA = np.pad(dA, ((0,0), (0,0), (0, (self.b_size-1))))\n",
        "    self.dA1 = np.zeros((self.n_samples, self.n_out_channels, self.b_size, self.dA.shape[-1]))\n",
        "    for i in range(self.b_size):\n",
        "        self.dA1[:, :, i] = np.roll(self.dA, i, axis=-1)\n",
        "    dX = np.sum(self.W[:, :, :, np.newaxis]*self.dA1[:, :, np.newaxis], axis=(1,3))\n",
        "    self.optimizer.update(self)\n",
        "    return dX"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-8d6zUhng1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d81c181-4853-4aa6-fb68-fa99dbf1a348"
      },
      "source": [
        "model_2 = AdvConv1d(3, SimpleInitializer(0.01), SGD(0.01), 2, 3, 0, 2)\n",
        "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) \n",
        "model_2.W = np.ones((3, 2, 3), dtype=float)\n",
        "model_2.b = np.array([1, 2, 3], dtype=float)\n",
        "model_2_forward = model_1.forward(x)\n",
        "model_2_forward"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[16., 22.],\n",
              "       [17., 23.],\n",
              "       [18., 24.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAPWT0sboDcM"
      },
      "source": [
        "#### [Problem 8] Learning and estimation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As0w4A-8IWAC"
      },
      "source": [
        "class SGD:\n",
        "  def __init__(self, lr):\n",
        "      self.lr = lr\n",
        "  \n",
        "  def update(self, layer):\n",
        "      layer.W -= self.lr * layer.dW\n",
        "      layer.b -= self.lr * layer.dB\n",
        "      return"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTJ_39vkoIRx"
      },
      "source": [
        "import math\n",
        "class ScratchCNNClassifier:\n",
        "    \n",
        "    def __init__(self, num_epoch=10, lr=0.01, batch_size=20, n_features=784, n_nodes1=400, n_nodes2=200, n_output=10, verbose=True, Activater=Tanh, Optimizer=SGD):\n",
        "        self.num_epoch = num_epoch\n",
        "        self.lr = lr\n",
        "        self.verbose = verbose  \n",
        "        self.batch_size = batch_size \n",
        "        self.n_features = n_features \n",
        "        self.n_nodes2 = n_nodes2 \n",
        "        self.n_output = n_output \n",
        "        self.Activater = Activater\n",
        "        if Activater == Sigmoid or Activater == Tanh:\n",
        "            self.Initializer = XavierInitializer\n",
        "        elif Activater == ReLU:\n",
        "            self.Initializer = HeInitializer\n",
        "        self.Optimizer = Optimizer\n",
        "    \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        self.val_enable = False\n",
        "        if X_val is not None:\n",
        "            self.val_enable = True\n",
        "        self.AdvConv1d = AdvConv1d(b_size=7, initializer=SimpleInitializer(0.01), optimizer=self.Optimizer(self.lr), n_in_channels=1, n_out_channels=1, pa=3, stride=2)\n",
        "        self.AdvConv1d.n_out = output_size_calc(X.shape[-1], self.AdvConv1d.b_size, self.AdvConv1d.pa, self.AdvConv1d.stride)\n",
        "        self.activation1 = self.Activater()\n",
        "        self.FC2 = FC(1*self.AdvConv1d.n_out, self.n_nodes2, self.Initializer(), self.Optimizer(self.lr))\n",
        "        self.activation2 = self.Activater()\n",
        "        self.FC3 = FC(self.n_nodes2, self.n_output, self.Initializer(), self.Optimizer(self.lr))\n",
        "        self.activation3 = Softmax()\n",
        "        \n",
        "        self.loss = []\n",
        "        self.loss_epoch = [self.activation3.loss_func(y, self.forward_propagation(X))]\n",
        "        \n",
        "        for _ in range(self.num_epoch):\n",
        "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
        "            self.iter = len(get_mini_batch)\n",
        "            for mini_X, mini_y in get_mini_batch:\n",
        "                self.forward_propagation(mini_X)\n",
        "                self.back_propagation(mini_X, mini_y)\n",
        "                self.loss.append(self.activation3.loss)\n",
        "            self.loss_epoch.append(self.activation3.loss_func(y, self.forward_propagation(X)))\n",
        "        \n",
        "    def predict(self, X):\n",
        "        return np.argmax(self.forward_propagation(X), axis=1)\n",
        "    \n",
        "    def forward_propagation(self, X):\n",
        "        A1 = self.AdvConv1d.forward(X)\n",
        "        A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
        "        Z1 = self.activation1.forward(A1)\n",
        "        A2 = self.FC2.forward(Z1)\n",
        "        Z2 = self.activation2.forward(A2)\n",
        "        A3 = self.FC3.forward(Z2)\n",
        "        Z3 = self.activation3.forward(A3)\n",
        "        return Z3\n",
        "        \n",
        "    def back_propagation(self, X, y_true):\n",
        "        dA3 = self.activation3.backward(y_true) \n",
        "        dZ2 = self.FC3.backward(dA3)\n",
        "        dA2 = self.activation2.backward(dZ2)\n",
        "        dZ1 = self.FC2.backward(dA2)\n",
        "        dA1 = self.activation1.backward(dZ1)\n",
        "        dA1 = dA1[:, np.newaxis]\n",
        "        dZ0 = self.AdvConv1d.backward(dA1) "
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yyBwczDucJL"
      },
      "source": [
        "# importing dependencies\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# get subsets\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# flattening the subsets\n",
        "X_train = X_train.reshape(-1,784)\n",
        "X_test = X_test.reshape(-1,784)\n",
        "\n",
        "# pre processing\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "# splitting our subsets into train and validation subsets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_val[:, np.newaxis])\n",
        "\n",
        "X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gkzmr-bWxX0k"
      },
      "source": [
        "model_3 = ScratchCNNClassifier(num_epoch=20, lr=0.01, batch_size=20, n_features=784, n_nodes1=400, n_nodes2=400, n_output=10, verbose=True, Activater=Tanh, Optimizer=SGD)\n",
        "model_3.fit(X_train_, y_train_)\n",
        "y_pred = model_3.predict(X_test)"
      ],
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp6BcWIFKXVy",
        "outputId": "c88e2d75-c4bc-49ae-d83c-51f6dd9df577",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9758"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    }
  ]
}